{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ROOT Logo](http://root.cern.ch/img/logos/ROOT_Logo/website-banner/website-banner-%28not%20root%20picture%29.jpg)\n",
    "<br />\n",
    "# **Distributing a TOTEM analysis with RDataFrame in Python**\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code shown in this notebook has been extracted from a real TOTEM analysis. Please refer to the [original source](https://github.com/jan-kaspar/analysis_elastic.6500GeV.beta90.10sigma) for further information.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The aim of this notebook is to present a complex real analysis rewritten from ROOT C++ to [RDataFrame](https://root.cern/doc/master/classROOT_1_1RDataFrame.html) which is able to run distributed using Spark as the backend framework.\n",
    "\n",
    "For the sake of completeness, the original code is splitted in two scripts with a clear purpose each:\n",
    "\n",
    "- `distill.cc`: \n",
    "    + Select branches to read\n",
    "    + Apply first filter to remove invalid data, thereby reducing extensively the amount of events to be processed\n",
    "    + Create a new ROOT file out of the reduced dataset  \n",
    "    + Long execution time $\\mathcal{O}(\\text{hours})$, hence is meant to be run once\n",
    "\n",
    "- `distributions.cc`: (to be run multiple times while tweaking parameters)\n",
    "    + Read branches from the root file created by distill.cc\n",
    "    + Apply multiple filters\n",
    "    + Produce TH1D, TH2D, TProfiles and TGraphs\n",
    "    + Save all results to a ROOT file \n",
    "    + Much shorter execution time, so it can be run multiple times while tunning parameters of the analysis.\n",
    "\n",
    "In this notebook, parts of the RDataFrame version of the code have been rearranged in order to run distributed through Spark. An extended version of this RDataFrame code is [hosted here](https://github.com/JavierCVilla/RDataFrame-Totem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the analysis\n",
    "\n",
    "First import the necessary packages. We need the `DistROOT` package to help us distribute the `RDataFrame` computation with **Spark**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TCanvas\n",
    "from DistROOT import DistTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next four cells include the code which will read, filter and produce histograms with the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Branches clasified by diagonal\n",
    "diagonals = {\n",
    "    # return a tuple: ([left] verticals in 45, [right] verticals in 56))\n",
    "    \"d45b_56t\"  : ([\"track_rp_5\", \"track_rp_21\", \"track_rp_25\"],\n",
    "                   [\"track_rp_104\", \"track_rp_120\", \"track_rp_124\"]),\n",
    "    \"ad45b_56b\" : ([\"track_rp_5\", \"track_rp_21\", \"track_rp_25\"],\n",
    "                   [\"track_rp_105\", \"track_rp_121\", \"track_rp_125\"]),\n",
    "    \"d45t_56b\"  : ([\"track_rp_4\", \"track_rp_20\", \"track_rp_24\"],\n",
    "                   [\"track_rp_105\", \"track_rp_121\", \"track_rp_125\"]),\n",
    "    \"ad45t_56t\" : ([\"track_rp_4\", \"track_rp_20\", \"track_rp_24\"],\n",
    "                   [\"track_rp_104\", \"track_rp_120\", \"track_rp_124\"])\n",
    "}\n",
    "\n",
    "# Data source files\n",
    "DS = {\n",
    "    'DS1'   : '4495', \n",
    "    'DS2'   : '4496', \n",
    "    'DS3'   : '4499', \n",
    "    'DS4'   : '4505', \n",
    "    'DS5'   : '4509', \n",
    "    'DS6'   : '4510', \n",
    "    'DS7'   : '4511'\n",
    "}\n",
    "\n",
    "# Select branches\n",
    "selected_diagonal = \"d45b_56t\"\n",
    "selected_ds = 'DS1'\n",
    "\n",
    "rp_left, rp_right = diagonals[selected_diagonal]\n",
    "\n",
    "# Columns per branch\n",
    "attributes = ['valid', 'x', 'y']\n",
    "\n",
    "full_branches = [\"{}.{}\".format(c,a) for a in attributes for c in rp_left+rp_right ]\n",
    "\n",
    "# Split left and right branch on valid, x and y\n",
    "valids = [ \"(unsigned int) {}\".format(v) for v in full_branches[0:6]]\n",
    "xs     = full_branches[6:12]\n",
    "ys     = full_branches[12:18]\n",
    "\n",
    "\n",
    "print(\"Selected branches: \\n\" + \"\\n\\t\".join(full_branches))\n",
    "\n",
    "# Filter to select valid branches\n",
    "filter_code = \"\"\"({0}.valid + {1}.valid + {2}.valid ) >= 2 &&\n",
    "({3}.valid + {4}.valid + {5}.valid ) >= 2\n",
    "\"\"\".format(*(rp_left+rp_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distill(rdf):\n",
    "    # Filter and define output branches\n",
    "    return rdf.Filter(filter_code)   \\\n",
    "       .Define(\"v_L_1_F\", valids[0]) \\\n",
    "       .Define(\"v_L_2_N\", valids[1]) \\\n",
    "       .Define(\"v_L_2_F\", valids[2]) \\\n",
    "       .Define(\"v_R_1_F\", valids[3]) \\\n",
    "       .Define(\"v_R_2_N\", valids[4]) \\\n",
    "       .Define(\"v_R_2_F\", valids[5]) \\\n",
    "       .Define(\"x_L_1_F\", xs[0]) \\\n",
    "       .Define(\"x_L_2_N\", xs[1]) \\\n",
    "       .Define(\"x_L_2_F\", xs[2]) \\\n",
    "       .Define(\"x_R_1_F\", xs[3]) \\\n",
    "       .Define(\"x_R_2_N\", xs[4]) \\\n",
    "       .Define(\"x_R_2_F\", xs[5]) \\\n",
    "       .Define(\"y_L_1_F\", ys[0]) \\\n",
    "       .Define(\"y_L_2_N\", ys[1]) \\\n",
    "       .Define(\"y_L_2_F\", ys[2]) \\\n",
    "       .Define(\"y_R_1_F\", ys[3]) \\\n",
    "       .Define(\"y_R_2_N\", ys[4]) \\\n",
    "       .Define(\"y_R_2_F\", ys[5]) \\\n",
    "       .Define(\"timestamp\",    \"(unsigned int) (event_info.timestamp - 1444860000)\") \\\n",
    "       .Define(\"run_num\",      \"(unsigned int) event_info.run_no\")                 \\\n",
    "       .Define(\"bunch_num\",    \"trigger_data.bunch_num\")            \\\n",
    "       .Define(\"event_num\",    \"trigger_data.event_num\")            \\\n",
    "       .Define(\"trigger_num\",  \"trigger_data.trigger_num\")          \\\n",
    "       .Define(\"trigger_bits\", \"trigger_data.input_status_bits\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binnings\n",
    "binnings = [\"ub\", \"ob-1-10-0.2\", \"ob-1-30-0.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distributions(rdf):\n",
    "    \n",
    "    hlist = []\n",
    "    \n",
    "    import ROOT\n",
    "    \n",
    "    # Load C++ headers\n",
    "    ROOT.gInterpreter.Declare('#include \"common_definitions.h\"')\n",
    "    ROOT.gInterpreter.Declare('#include \"parameters_global.h\"')\n",
    "    ROOT.gInterpreter.Declare('#include \"common_algorithms.h\"')\n",
    "    ROOT.gInterpreter.Declare('#include \"parameters.h\"')\n",
    "    ROOT.gInterpreter.Declare('#include \"common.h\"')\n",
    "    \n",
    "    detailsLevel         = 0 # 0: no details, 1: some details, >= 2 all details\n",
    "    overrideCutSelection = False # whether the default cut selection should be overriden by the command-line selection\n",
    "    cutSelectionString   = None\n",
    "    outputDir            = \".\"\n",
    "    inputDir             = \".\"\n",
    "    input_n_si           = 4.0\n",
    "    time_group_divisor   = 0\n",
    "    time_group_remainder = 0\n",
    "    event_group_divisor  = 0\n",
    "    event_group_index    = 0\n",
    "    evIdxStep            = 1\n",
    "    maxTaggedEvents      = 0\n",
    "    \n",
    "    M_PI = 3.14159265358979323846264338328      # Pi\n",
    "    \n",
    "    # init diagonal settings\n",
    "    ROOT.Init(\"45b_56t\")\n",
    "\n",
    "    # default parameters\n",
    "    input_n_si = 4.0\n",
    "\n",
    "    # select cuts\n",
    "    ROOT.anal.BuildCuts()\n",
    "    ROOT.anal.n_si = input_n_si\n",
    "\n",
    "    # alignment init\n",
    "    for i,_ in  enumerate(ROOT.alignmentSources):\n",
    "        ROOT.alignmentSources[i].Init()\n",
    "\n",
    "    # get time-dependent corrections\n",
    "    corrg_pileup = None\n",
    "    if ROOT.anal.use_pileup_efficiency_fits:\n",
    "        path = inputDir + \"/pileup_fit_combined.root\"\n",
    "        puF = ROOT.TFile.Open(path)\n",
    "        if not os.path.exists(puF):\n",
    "            print(\"ERROR: pile-up correction file `%s' cannot be opened.\\n\" % path);\n",
    "        if diagonal == \"d45b_56t\":\n",
    "            #corrg_pileup = (TGraph *) puF.Get(\"45b_56t/dgn\");\n",
    "            corrg_pileup = puF.Get(\"45b_56t/dgn\")\n",
    "        if diagonal == \"d45t_56b\":\n",
    "            #corrg_pileup = (TGraph *) puF.Get(\"45b_56t/dgn\");\n",
    "            corrg_pileup = puF.Get(\"45t_56b/dgn\")\n",
    "\n",
    "    # get th_y* dependent efficiency correction\n",
    "    f_3outof4_efficiency_L_F = None\n",
    "    f_3outof4_efficiency_L_N = None\n",
    "    f_3outof4_efficiency_R_N = None\n",
    "    f_3outof4_efficiency_R_F = None\n",
    "\n",
    "    if ROOT.anal.use_3outof4_efficiency_fits:\n",
    "        path = inputDir + \"/eff3outof4_details_fit_old.root\"\n",
    "        effFile = ROOT.TFile.Open(path)\n",
    "        if (os.path.exists(effFile)):\n",
    "            print(\"ERROR: 3-out-of-4 efficiency file `%s' cannot be opened.\\n\" % path)\n",
    "\n",
    "        diagonal = selected_diagonal;\n",
    "        f_3outof4_efficiency_L_F = effFile.Get(diagonal + \"/L_F/fit\")\n",
    "        f_3outof4_efficiency_L_N = effFile.Get(diagonal + \"/L_N/fit\")\n",
    "        f_3outof4_efficiency_R_N = effFile.Get(diagonal + \"/R_N/fit\")\n",
    "        f_3outof4_efficiency_R_F = effFile.Get(diagonal + \"/R_F/fit\")\n",
    "        \n",
    "        print(\"\\n>> using 3-out-of-4 fits: %s, %s, %s, %s\\n\" % (f_3outof4_efficiency_L_F, \n",
    "                                                                f_3outof4_efficiency_L_N, \n",
    "                                                                f_3outof4_efficiency_R_N, \n",
    "                                                                f_3outof4_efficiency_R_F))\n",
    "\n",
    "    # book metadata histograms\n",
    "    ROOT.timestamp_bins = ROOT.timestamp_max - ROOT.timestamp_min + 1.\n",
    "\n",
    "    # Create bh_t_* hists\n",
    "    bh_t_Nev_before = dict()\n",
    "    bh_t_Nev_after_no_corr = dict()\n",
    "    bh_t_before = dict()\n",
    "    bh_t_after = dict()\n",
    "    bh_t_after_no_corr = dict()\n",
    "    bp_t_phi_corr = dict()\n",
    "    bp_t_full_corr = dict()\n",
    "\n",
    "    binning_setup = dict()\n",
    "    for b in binnings:\n",
    "        binning = ROOT.BuildBinningRDF(ROOT.anal, b)\n",
    "        binning_setup[b] = binning\n",
    "\n",
    "    # zero counters\n",
    "    n_ev_full = 0;\n",
    "    n_ev_cut = dict();\n",
    "    for ci in range(ROOT.anal.N_cuts):\n",
    "        n_ev_cut[ci] = 0\n",
    "\n",
    "    th_min = 1E100;\n",
    "    th_y_L_min = +1E100; th_y_R_min = +1E100\n",
    "\n",
    "    N_anal=0; N_anal_zeroBias=0;\n",
    "    N_zeroBias_el=0; N_zeroBias_el_RP_trig=0;\n",
    "    N_4outof4=0; N_el=0;\n",
    "    N_el_T2trig=0; N_4outof4_T2trig=0;\n",
    "    N_el_raw=0;\n",
    "\n",
    "    #########################################################\n",
    "    ###### FILTER, BUILD HISTOGRAMS - START EVENT LOOP ######\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    # Initial cuts\n",
    "    f1 = rdf.Filter(\"! SkipTime( timestamp )\", 'check time - selected')\n",
    "\n",
    "    if time_group_divisor != 0:\n",
    "        f1 = f1.Filter(\"! SkipTimeInterval( timestamp, %s, %s )\".format(time_group_divisor, time_group_remainder),\n",
    "                        'time interval')\n",
    "\n",
    "    # Diagonal cut (L831)\n",
    "    f2 = f1.Filter(\"v_L_2_F && v_L_2_N && v_R_2_F && v_R_2_N\", 'allDiagonalRPs')\n",
    "\n",
    "    model = (\"h_timestamp_dgn\", \";timestamp;rate   (Hz)\", \n",
    "             int(ROOT.timestamp_bins), ROOT.timestamp_min-0.5, ROOT.timestamp_max+0.5)\n",
    "    h_timestamp_dgn = f2.Histo1D(model, \"timestamp\")\n",
    "\n",
    "    # Not cut for this filter in original code\n",
    "    f_zerobias = f2.Filter(\"! ((trigger_bits & 512) != 0)\", 'zero_bias_event')\n",
    "\n",
    "    xs = [\"x_L_1_F\", \"x_L_2_N\", \"x_L_2_F\", \"x_R_1_F\", \"x_R_2_N\", \"x_R_2_F\"]\n",
    "    ys = [\"y_L_1_F\", \"y_L_2_N\", \"y_L_2_F\", \"y_R_1_F\", \"y_R_2_N\", \"y_R_2_F\"]\n",
    "\n",
    "    # Apply fine alignment (L 852)\n",
    "    r2 = f2.Define(\"h_al\", \"ApplyFineAlignment( timestamp ,{}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\".format(*(xs+ys) )) \\\n",
    "        .Define(\"h_al_x_L_1_F\", \"h_al.L_1_F.x\") \\\n",
    "        .Define(\"h_al_x_L_2_N\", \"h_al.L_2_N.x\") \\\n",
    "        .Define(\"h_al_x_L_2_F\", \"h_al.L_2_F.x\") \\\n",
    "        .Define(\"h_al_y_L_1_F\", \"h_al.L_1_F.y\") \\\n",
    "        .Define(\"h_al_y_L_2_N\", \"h_al.L_2_N.y\") \\\n",
    "        .Define(\"h_al_y_L_2_F\", \"h_al.L_2_F.y\") \\\n",
    "        .Define(\"h_al_x_R_1_F\", \"h_al.R_1_F.x\") \\\n",
    "        .Define(\"h_al_x_R_2_N\", \"h_al.R_2_N.x\") \\\n",
    "        .Define(\"h_al_x_R_2_F\", \"h_al.R_2_F.x\") \\\n",
    "        .Define(\"h_al_y_R_1_F\", \"h_al.R_1_F.y\") \\\n",
    "        .Define(\"h_al_y_R_2_N\", \"h_al.R_2_N.y\") \\\n",
    "        .Define(\"h_al_y_R_2_F\", \"h_al.R_2_F.y\") \\\n",
    "\n",
    "    # fill pre-selection histograms\n",
    "    al_nosel_models = [\n",
    "        (\"h_y_L_1_F_vs_x_L_1_F_al_nosel\", \";x^{L,1,F};y^{L,1,F}\", 150, -15., 15., 300, -30., +30.),\n",
    "        (\"h_y_L_2_N_vs_x_L_2_N_al_nosel\", \";x^{L,2,N};y^{L,2,N}\", 150, -15., 15., 300, -30., +30.),\n",
    "        (\"h_y_L_2_F_vs_x_L_2_F_al_nosel\", \";x^{L,2,F};y^{L,2,F}\", 150, -15., 15., 300, -30., +30.),\n",
    "        (\"h_y_R_1_F_vs_x_R_1_F_al_nosel\", \";x^{R,1,F};y^{R,1,F}\", 150, -15., 15., 300, -30., +30.),\n",
    "        (\"h_y_R_2_N_vs_x_R_2_N_al_nosel\", \";x^{R,2,N};y^{R,2,N}\", 150, -15., 15., 300, -30., +30.),\n",
    "        (\"h_y_R_2_F_vs_x_R_2_F_al_nosel\", \";x^{R,2,F};y^{R,2,F}\", 150, -15., 15., 300, -30., +30.)\n",
    "    ]\n",
    "\n",
    "    h_y_L_1_F_vs_x_L_1_F_al_nosel = r2.Histo2D(al_nosel_models[0], \"h_al_x_L_1_F\", \"h_al_y_L_1_F\")\n",
    "    h_y_L_2_N_vs_x_L_2_N_al_nosel = r2.Histo2D(al_nosel_models[1], \"h_al_x_L_2_N\", \"h_al_y_L_2_N\")\n",
    "    h_y_L_2_F_vs_x_L_2_F_al_nosel = r2.Histo2D(al_nosel_models[2], \"h_al_x_L_2_F\", \"h_al_y_L_2_F\")\n",
    "    h_y_R_1_F_vs_x_R_1_F_al_nosel = r2.Histo2D(al_nosel_models[3], \"h_al_x_R_1_F\", \"h_al_y_R_1_F\")\n",
    "    h_y_R_2_N_vs_x_R_2_N_al_nosel = r2.Histo2D(al_nosel_models[4], \"h_al_x_R_2_N\", \"h_al_y_R_2_N\")\n",
    "    h_y_R_2_F_vs_x_R_2_F_al_nosel = r2.Histo2D(al_nosel_models[5], \"h_al_x_R_2_F\", \"h_al_y_R_2_F\")\n",
    "\n",
    "    # run reconstruction \n",
    "    ### kinematics struct\n",
    "    r3 = r2.Define(\"kinematics\", 'DoReconstruction( h_al )')\n",
    "\n",
    "    r4 =  r3.Define(\"k_th_x_R\",        \"kinematics.th_x_R\")   \\\n",
    "            .Define(\"k_th_y_R\",        \"kinematics.th_y_R\")   \\\n",
    "            .Define(\"k_th_x_L\",        \"kinematics.th_x_L\")   \\\n",
    "            .Define(\"k_th_y_L\",        \"kinematics.th_y_L\")   \\\n",
    "            .Define(\"k_th_x\",          \"kinematics.th_x\")     \\\n",
    "            .Define(\"k_th_y\",          \"kinematics.th_y\")     \\\n",
    "            .Define(\"minus_k_th_y\",    \"- kinematics.th_y\")   \\\n",
    "            .Define(\"k_vtx_x\",         \"kinematics.vtx_x\")    \\\n",
    "            .Define(\"k_vtx_x_L\",       \"kinematics.vtx_x_L\")  \\\n",
    "            .Define(\"k_vtx_x_R\",       \"kinematics.vtx_x_R\")  \\\n",
    "            .Define(\"k_vtx_y\",         \"kinematics.vtx_y\")    \\\n",
    "            .Define(\"k_vtx_y_L\",       \"kinematics.vtx_y_L\")  \\\n",
    "            .Define(\"k_vtx_y_R\",       \"kinematics.vtx_y_R\")  \\\n",
    "            .Define(\"k_th_y_L_F\",      \"kinematics.th_y_L_F\") \\\n",
    "            .Define(\"k_th_y_L_N\",      \"kinematics.th_y_L_N\") \\\n",
    "            .Define(\"k_th_y_R_F\",      \"kinematics.th_y_R_F\") \\\n",
    "            .Define(\"k_th_y_R_N\",      \"kinematics.th_y_R_N\") \\\n",
    "            .Define(\"k_th_x_diffLR\",   \"kinematics.th_x_R - kinematics.th_x_L\")     \\\n",
    "            .Define(\"k_th_y_diffLR\",   \"kinematics.th_y_R - kinematics.th_y_L\")     \\\n",
    "            .Define(\"k_th_x_diffLF\",   \"kinematics.th_x_L - kinematics.th_x\")       \\\n",
    "            .Define(\"k_th_x_diffRF\",   \"kinematics.th_x_R - kinematics.th_x\")       \\\n",
    "            .Define(\"k_th_y_L_diffNF\", \"kinematics.th_y_L_F - kinematics.th_y_L_N\") \\\n",
    "            .Define(\"k_th_y_R_diffNF\", \"kinematics.th_y_R_F - kinematics.th_y_R_N\") \\\n",
    "            .Define(\"k_vtx_x_diffLR\",  \"kinematics.vtx_x_R - kinematics.vtx_x_L\")   \\\n",
    "            .Define(\"k_vtx_y_diffLR\",  \"kinematics.vtx_y_R - kinematics.vtx_y_L\")   \\\n",
    "            .Define(\"k_t\",             \"kinematics.t\")                              \\\n",
    "            .Define(\"k_th\",            \"kinematics.th\")                             \\\n",
    "            .Define(\"k_phi\",           \"kinematics.phi\")\n",
    "\n",
    "    # cut evaluation\n",
    "    r5 = r4.Define(\"cutdata\", \"EvaluateCutsRDF( h_al, kinematics, anal )\")\n",
    "\n",
    "    # Elastic cut\n",
    "    f4 = r5.Filter(\"cutdata.select\", \"elastic cut\")\n",
    "\n",
    "    # Fill raw histograms\n",
    "    model = (\"h_timestamp_sel\", \";timestamp;rate   (Hz)\", \n",
    "             int(ROOT.timestamp_bins), ROOT.timestamp_min-0.5, ROOT.timestamp_max+0.5)\n",
    "    h_timestamp_sel = f4.Histo1D(model, \"timestamp\");\n",
    "\n",
    "    # fill histograms\n",
    "    noal_sel_models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_y_L_1_F_vs_x_L_1_F_noal_sel\", \";x^{L,1,F};y^{L,1,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_L_2_N_vs_x_L_2_N_noal_sel\", \";x^{L,2,N};y^{L,2,N}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_L_2_F_vs_x_L_2_F_noal_sel\", \";x^{L,2,F};y^{L,2,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_1_F_vs_x_R_1_F_noal_sel\", \";x^{R,1,F};y^{R,1,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_2_N_vs_x_R_2_N_noal_sel\", \";x^{R,2,N};y^{R,2,N}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_2_F_vs_x_R_2_F_noal_sel\", \";x^{R,2,F};y^{R,2,F}\", 100, -3., +3., 300, -30., +30.)\n",
    "    ])\n",
    "\n",
    "    h_y_L_1_F_vs_x_L_1_F_noal_sel = f4.Histo2D(noal_sel_models[0], \"x_L_1_F\", \"y_L_1_F\")\n",
    "    h_y_L_2_N_vs_x_L_2_N_noal_sel = f4.Histo2D(noal_sel_models[1], \"x_L_2_N\", \"y_L_2_N\")\n",
    "    h_y_L_2_F_vs_x_L_2_F_noal_sel = f4.Histo2D(noal_sel_models[2], \"x_L_2_F\", \"y_L_2_F\")\n",
    "    h_y_R_1_F_vs_x_R_1_F_noal_sel = f4.Histo2D(noal_sel_models[3], \"x_R_1_F\", \"y_R_1_F\")\n",
    "    h_y_R_2_N_vs_x_R_2_N_noal_sel = f4.Histo2D(noal_sel_models[4], \"x_R_2_N\", \"y_R_2_N\")\n",
    "    h_y_R_2_F_vs_x_R_2_F_noal_sel = f4.Histo2D(noal_sel_models[5], \"x_R_2_F\", \"y_R_2_F\")\n",
    "\n",
    "    al_sel_models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_y_L_1_F_vs_x_L_1_F_al_sel\", \";x^{L,1,F};y^{L,1,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_L_2_N_vs_x_L_2_N_al_sel\", \";x^{L,2,N};y^{L,2,N}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_L_2_F_vs_x_L_2_F_al_sel\", \";x^{L,2,F};y^{L,2,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_1_F_vs_x_R_1_F_al_sel\", \";x^{R,1,F};y^{R,1,F}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_2_N_vs_x_R_2_N_al_sel\", \";x^{R,2,N};y^{R,2,N}\", 100, -3., +3., 300, -30., +30.),\n",
    "        (\"h_y_R_2_F_vs_x_R_2_F_al_sel\", \";x^{R,2,F};y^{R,2,F}\", 100, -3., +3., 300, -30., +30.)\n",
    "    ])\n",
    "\n",
    "    h_y_L_1_F_vs_x_L_1_F_al_sel = f4.Histo2D(al_sel_models[0], \"h_al_x_L_1_F\", \"h_al_y_L_1_F\")\n",
    "    h_y_L_2_N_vs_x_L_2_N_al_sel = f4.Histo2D(al_sel_models[1], \"h_al_x_L_2_N\", \"h_al_y_L_2_N\")\n",
    "    h_y_L_2_F_vs_x_L_2_F_al_sel = f4.Histo2D(al_sel_models[2], \"h_al_x_L_2_F\", \"h_al_y_L_2_F\")\n",
    "    h_y_R_1_F_vs_x_R_1_F_al_sel = f4.Histo2D(al_sel_models[3], \"h_al_x_R_1_F\", \"h_al_y_R_1_F\")\n",
    "    h_y_R_2_N_vs_x_R_2_N_al_sel = f4.Histo2D(al_sel_models[4], \"h_al_x_R_2_N\", \"h_al_y_R_2_N\")\n",
    "    h_y_R_2_F_vs_x_R_2_F_al_sel = f4.Histo2D(al_sel_models[5], \"h_al_x_R_2_F\", \"h_al_y_R_2_F\")\n",
    "\n",
    "    # Line 1157 (k.th_x_R - k.th_x_L)\n",
    "    #           (k.th_y_R - k.th_y_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"th_x_diffLR\", \";#theta_{x}^{R} - #theta_{x}^{L}\", 1000, -500E-6, +500E-6),\n",
    "        (\"th_y_diffLR\", \";#theta_{y}^{R} - #theta_{y}^{L}\", 500, -50E-6, +50E-6)\n",
    "    ])\n",
    "    th_x_diffLR = f4.Histo1D(models[0], \"k_th_x_diffLR\")\n",
    "    th_y_diffLR = f4.Histo1D(models[1], \"k_th_y_diffLR\")\n",
    "\n",
    "    # Line 1160 (k.th_x_L - k.th_x)\n",
    "    #           (k.th_x_R - k.th_x)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"th_x_diffLF\", \";#theta_{x}^{L} - #theta_{x}\", 400, -200E-6, +200E-6),\n",
    "        (\"th_x_diffRF\", \";#theta_{x}^{R} - #theta_{x}\", 400, -200E-6, +200E-6)\n",
    "    ])\n",
    "    th_x_diffLF = f4.Histo1D(models[0], \"k_th_x_diffLF\")\n",
    "    th_x_diffRF = f4.Histo1D(models[1], \"k_th_x_diffRF\")\n",
    "\n",
    "    # Line 1163 (k.th_x, k.th_x_R - k.th_x_L)\n",
    "    #           (k.th_y, k.th_y_R - k.th_y_L)\n",
    "    #           (k.vtx_x, k.th_x_R - k.th_x_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_th_x_diffLR_vs_th_x\", \";#theta_{x};#theta_{x}^{R} - #theta_{x}^{L}\", 100, -300E-6, +300E-6, 120, -120E-6, +120E-6),\n",
    "        (\"h_th_y_diffLR_vs_th_y\", \";#theta_{y};#theta_{y}^{R} - #theta_{y}^{L}\", 100, -500E-6, +500E-6, 120, -120E-6, +120E-6),\n",
    "        (\"h_th_x_diffLR_vs_vtx_x\", \";vtx_{x};#theta_{x}^{R} - #theta_{x}^{L}\", 100, -300E-3, +300E-3, 120, -120E-6, +120E-6)\n",
    "    ])\n",
    "    h_th_x_diffLR_vs_th_x  = f4.Histo2D(models[0], \"k_th_x\", \"k_th_x_diffLR\")\n",
    "    h_th_y_diffLR_vs_th_y  = f4.Histo2D(models[1], \"k_th_y\", \"k_th_y_diffLR\")\n",
    "    h_th_x_diffLR_vs_vtx_x = f4.Histo2D(models[2], \"k_vtx_x\", \"k_th_x_diffLR\")\n",
    "\n",
    "\n",
    "    # Line 1188 (k.th_x_L, k.th_y_L)\n",
    "    #           (k.th_x_R, k.th_y_R)\n",
    "    #           (k.th_x, k.th_y)\n",
    "    models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_th_y_L_vs_th_x_L\", \";#theta_{x}^{L};#theta_{y}^{L}\", 100, -115E-6, +11E-5, 100, 22E-6, +102E-6),\n",
    "        (\"h_th_y_R_vs_th_x_R\", \";#theta_{x}^{R};#theta_{y}^{R}\", 100, -125E-6, +12E-5, 100, 27E-6, +102E-6),\n",
    "        (\"h_th_y_vs_th_x\", \";#theta_{x};#theta_{y}\", 100, -300E-6, +300E-6, 100, -150E-6, +150E-6)\n",
    "    ])\n",
    "    h_th_y_L_vs_th_x_L = f4.Histo2D(models[0], \"k_th_x_L\", \"k_th_y_L\")\n",
    "    h_th_y_R_vs_th_x_R = f4.Histo2D(models[1], \"k_th_x_R\", \"k_th_y_R\")\n",
    "    h_th_y_vs_th_x     = f4.Histo2D(models[2], \"k_th_x\", \"k_th_y\")\n",
    "\n",
    "    # Line 1199 (k.th_y_R, k.th_y_L)\n",
    "    model = ROOT.ROOT.RDF.TH2DModel(\"h_th_y_L_vs_th_y_R\", \";#theta_{y}^{R};#theta_{y}^{L}\",\n",
    "                                    300, -150E-6, +150E-6, 300, -150E-6, +150E-6)\n",
    "    h_th_y_L_vs_th_y_R = f4.Histo2D(model, \"k_th_y_R\", \"k_th_y_L\")\n",
    "\n",
    "    # Line 1203: (k.th_x)\n",
    "    #            (k.th_y)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_th_x\", \";#theta_{x}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_y\", \";#theta_{y}\", 250, -500E-6, +500E-6)\n",
    "    ])\n",
    "    h_th_x     = f4.Histo1D(models[0], \"k_th_x\")\n",
    "    h_th_y     = f4.Histo1D(models[1], \"k_th_y\")\n",
    "\n",
    "    # Line 1205: (-k.th_y)\n",
    "    model = (\"h_th_y_flipped\", \";#theta_{y}\", 250, -500E-6, +500E-6)\n",
    "    h_th_y_flipped = f4.Histo1D(model, \"minus_k_th_y\")\n",
    "\n",
    "    # Line 1207: (k.th_x_L)\n",
    "    #            (k.th_x_R)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_th_x_L\", \";#theta_{x}^{L}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_x_R\", \";#theta_{x}^{R}\", 250, -500E-6, +500E-6)\n",
    "    ])\n",
    "    h_th_x_L   = f4.Histo1D(models[0], \"k_th_x_L\")\n",
    "    h_th_x_R   = f4.Histo1D(models[1], \"k_th_x_R\")\n",
    "\n",
    "    # Line 1210: (k.th_y_L)\n",
    "    #            (k.th_y_R)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_th_y_L\", \";#theta_{y}^{L}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_y_R\", \";#theta_{y}^{R}\", 250, -500E-6, +500E-6)\n",
    "    ])\n",
    "    h_th_y_L   = f4.Histo1D(models[0], \"k_th_y_L\")\n",
    "    h_th_y_R   = f4.Histo1D(models[1], \"k_th_y_R\")\n",
    "\n",
    "    # Line 1213: (k.th_y_L_F)\n",
    "    #            (k.th_y_L_N)\n",
    "    #            (k.th_y_R_N)\n",
    "    #            (k.th_y_R_F)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_th_y_L_F\", \";#theta_{y}^{L_F}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_y_L_N\", \";#theta_{y}^{L_N}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_y_R_N\", \";#theta_{y}^{R_N}\", 250, -500E-6, +500E-6),\n",
    "        (\"h_th_y_R_F\", \";#theta_{y}^{R_F}\", 250, -500E-6, +500E-6)\n",
    "    ])\n",
    "    h_th_y_L_F = f4.Histo1D(models[0], \"k_th_y_L_F\")\n",
    "    h_th_y_L_N = f4.Histo1D(models[1], \"k_th_y_L_N\")\n",
    "    h_th_y_R_N = f4.Histo1D(models[2], \"k_th_y_R_N\")\n",
    "    h_th_y_R_F = f4.Histo1D(models[3], \"k_th_y_R_F\")\n",
    "\n",
    "\n",
    "    # fill vertex histograms\n",
    "\n",
    "    # Line 1220 (k.vtx_x)\n",
    "    #           (k.vtx_x_L)\n",
    "    #           (k.vtx_x_R)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_vtx_x\", \";x^{*}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_x_L\", \";x^{*,L}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_x_R\", \";x^{*,R}\", 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x    = f4.Histo1D(models[0], \"k_vtx_x\")\n",
    "    h_vtx_x_L  = f4.Histo1D(models[1], \"k_vtx_x_L\")\n",
    "    h_vtx_x_R  = f4.Histo1D(models[2], \"k_vtx_x_R\")\n",
    "\n",
    "    # Line 1224 (k.vtx_y)\n",
    "    #           (k.vtx_y_L)\n",
    "    #           (k.vtx_y_R)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_vtx_y\", \";y^{*}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_L\", \";y^{*,L}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_R\", \";y^{*,R}\", 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_y    = f4.Histo1D(models[0], \"k_vtx_y\")\n",
    "    h_vtx_y_L  = f4.Histo1D(models[1], \"k_vtx_y_L\")\n",
    "    h_vtx_y_R  = f4.Histo1D(models[2], \"k_vtx_y_R\")\n",
    "\n",
    "    # Line 1228:\n",
    "    #            (k.vtx_x_R, k.vtx_x_L)\n",
    "    #            (k.vtx_y_R, k.vtx_y_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_vtx_x_L_vs_vtx_x_R\", \";x^{*,R};x^{*,L}\", 100, -0.5, +0.5, 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_L_vs_vtx_y_R\", \";y^{*,R};y^{*,L}\", 100, -0.5, +0.5, 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x_L_vs_vtx_x_R = f4.Histo2D(models[0], \"k_vtx_x_R\", \"k_vtx_x_L\")\n",
    "    h_vtx_y_L_vs_vtx_y_R = f4.Histo2D(models[1], \"k_vtx_y_R\", \"k_vtx_y_L\")\n",
    "\n",
    "    # Line 1231:\n",
    "    #            (k.th_x_L, k.vtx_x_L)\n",
    "    #            (k.th_x_R, k.vtx_x_R)\n",
    "    #            (k.th_y_L, k.vtx_y_L)\n",
    "    #            (k.th_y_R, k.vtx_y_R)\n",
    "    models = map(ROOT.ROOT.RDF.TH2DModel, [\n",
    "        (\"h_vtx_x_L_vs_th_x_L\", \";#theta_{x}^{L};x^{*,L}\", 100, -600E-6, +600E-6, 100, -0.5, +0.5),\n",
    "        (\"h_vtx_x_R_vs_th_x_R\", \";#theta_{x}^{R};x^{*,R}\", 100, -600E-6, +600E-6, 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_L_vs_th_y_L\", \";#theta_{y}^{L};y^{*,L}\", 100, -600E-6, +600E-6, 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_R_vs_th_y_R\", \";#theta_{y}^{R};y^{*,R}\", 100, -600E-6, +600E-6, 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x_L_vs_th_x_L = f4.Histo2D(models[0], \"k_th_x_L\", \"k_vtx_x_L\")\n",
    "    h_vtx_x_R_vs_th_x_R = f4.Histo2D(models[1], \"k_th_x_R\", \"k_vtx_x_R\")\n",
    "    h_vtx_y_L_vs_th_y_L = f4.Histo2D(models[2], \"k_th_y_L\", \"k_vtx_y_L\")\n",
    "    h_vtx_y_R_vs_th_y_R = f4.Histo2D(models[3], \"k_th_y_R\", \"k_vtx_y_R\")\n",
    "\n",
    "    # Line 1236:\n",
    "    #           (k.vtx_x_R - k.vtx_x_L)\n",
    "    #           (k.vtx_y_R - k.vtx_y_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_vtx_x_diffLR\", \";x^{*,R} - x^{*,L}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_diffLR\", \";y^{*,R} - y^{*,L}\", 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x_diffLR = f4.Histo1D(models[0], \"k_vtx_x_diffLR\");\n",
    "    h_vtx_y_diffLR = f4.Histo1D(models[1], \"k_vtx_y_diffLR\");\n",
    "\n",
    "    # Line 1239:\n",
    "    #           (k.th_x, k.vtx_x_R - k.vtx_x_L)\n",
    "    #           (k.th_y, k.vtx_y_R - k.vtx_y_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH1DModel, [\n",
    "        (\"h_vtx_x_diffLR\", \";x^{*,R} - x^{*,L}\", 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_diffLR\", \";y^{*,R} - y^{*,L}\", 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x_diffLR_vs_th_x = f4.Histo1D(models[0], \"k_th_x\", \"k_vtx_x_diffLR\");\n",
    "    h_vtx_y_diffLR_vs_th_y = f4.Histo1D(models[1], \"k_th_y\", \"k_vtx_y_diffLR\");\n",
    "\n",
    "    # Line 1245:\n",
    "    #           (k.vtx_x_R, k.vtx_x_R - k.vtx_x_L)\n",
    "    #           (k.vtx_y_R, k.vtx_y_R - k.vtx_y_L)\n",
    "    models = map(ROOT.ROOT.RDF.TH2DModel,[\n",
    "        (\"h_vtx_x_diffLR_vs_vtx_x_R\", \";x^{*,R};x^{*,R} - x^{*,L}\", 100, -0.5, +0.5, 100, -0.5, +0.5),\n",
    "        (\"h_vtx_y_diffLR_vs_vtx_y_R\", \";y^{*,R};y^{*,R} - y^{*,L}\", 100, -0.5, +0.5, 100, -0.5, +0.5)\n",
    "    ])\n",
    "    h_vtx_x_diffLR_vs_vtx_x_R = f4.Histo2D(models[0], \"k_vtx_x_R\", \"k_vtx_y_diffLR\");\n",
    "    h_vtx_y_diffLR_vs_vtx_y_R = f4.Histo2D(models[1], \"k_vtx_y_R\", \"k_vtx_y_diffLR\");\n",
    "\n",
    "    # Define normalization and norm_corr colums\n",
    "    r6 = f4.Define(\"norm_corr\",     \"getNorm_corr( timestamp )\" ) \\\n",
    "        .Define(\"normalization\", \"getNormalization( norm_corr )\")\n",
    "\n",
    "    \n",
    "    # calculate acceptance divergence correction\n",
    "    r7 = r6.Define(\"correction\", \"CalculateAcceptanceCorrectionsRDF( th_y_sign, kinematics, anal )\") \\\n",
    "        .Define(\"corr\",       \"correction.corr\") \\\n",
    "        .Define(\"div_corr\",   \"correction.div_corr\") \\\n",
    "        .Define(\"one\",        \"One()\")\n",
    "\n",
    "    # Line 1406\n",
    "    for bi in binnings:\n",
    "        bis = binning_setup[bi]\n",
    "\n",
    "        model = ROOT.RDF.TH1DModel(\"h_t_Nev_before\", \";|t|;events per bin\", bis.N_bins, bis.bin_edges)\n",
    "        bh_t_Nev_before[bi] = r7.Histo1D(model, \"k_t\", \"one\");\n",
    "\n",
    "        model = ROOT.RDF.TH1DModel(\"h_t_before\", \";|t|\", bis.N_bins, bis.bin_edges)\n",
    "        bh_t_before[bi] = r7.Histo1D(model, \"k_t\", \"one\");\n",
    "\n",
    "\n",
    "    # Line 1412\n",
    "    model = ROOT.RDF.TH2DModel(\"h_th_y_vs_th_x_before\", \";#theta_{x};#theta_{y}\", 150, -300E-6, +300E-6, 150, -150E-6, +150E-6)\n",
    "    h_th_y_vs_th_x_before = r7.Histo2D(model, \"k_th_x\", \"k_th_y\", \"one\");\n",
    "\n",
    "    # Line 1414\n",
    "    # Filter skip\n",
    "    f5 = r7.Filter(\"! correction.skip\", \"acceptance correction\")\n",
    "\n",
    "    # Line 1429\n",
    "    for bi in binnings:\n",
    "        bis = binning_setup[bi]\n",
    "\n",
    "        model = ROOT.RDF.TH1DModel(\"h_t_Nev_after_no_corr\", \";|t|;events per bin\", bis.N_bins, bis.bin_edges)\n",
    "        bh_t_Nev_after_no_corr[bi] = f5.Histo1D(model, \"k_t\", \"one\");\n",
    "\n",
    "        model = ROOT.RDF.TH1DModel(\"h_t_after_no_corr\", \";|t|\", bis.N_bins, bis.bin_edges)\n",
    "        bh_t_after_no_corr[bi] = f5.Histo1D(model, \"k_t\", \"one\");\n",
    "\n",
    "        model = ROOT.RDF.TH1DModel(\"h_t_after\", \";|t|\", bis.N_bins, bis.bin_edges)\n",
    "        bh_t_after[bi] = f5.Histo1D(model, \"k_t\", \"corr\");\n",
    "\n",
    "    # Line 1435\n",
    "    model = ROOT.RDF.TH2DModel(\"h_th_y_vs_th_x_after\", \";#theta_{x};#theta_{y}\", 150, -300E-6, +300E-6, 150, -150E-6, +150E-6);\n",
    "    h_th_y_vs_th_x_after = f5.Histo2D(model, \"k_th_x\", \"k_th_y\", \"div_corr\");\n",
    "\n",
    "    # Line 1435\n",
    "    model = ROOT.RDF.TH2DModel(\"h_th_vs_phi_after\", \";#phi;#theta\", 50, -M_PI, +M_PI, 50, 150E-6, 550E-6);\n",
    "    h_th_vs_phi_after = f5.Histo2D(model, \"k_phi\", \"k_th\", \"div_corr\");\n",
    "\n",
    "    # Line 1441\n",
    "    # apply normalization\n",
    "    model = ROOT.ROOT.RDF.TH1DModel(\"h_t_normalized\", \";|t|\",128, 0., 4.)\n",
    "    bh_t_normalized_ob_1_30_02 = f5.Define(\"corr_norm\", \"corr * normalization\") \\\n",
    "                                .Histo1D(model, \"k_t\", \"corr_norm\")\n",
    "\n",
    "    # Line 1445\n",
    "    model = ROOT.RDF.TH2DModel(\"h_th_y_vs_th_x_normalized\", \";#theta_{x};#theta_{y}\", 150, -600E-6, +600E-6, 150, -600E-6, +600E-6);\n",
    "    h_th_y_vs_th_x_normalized = f5.Define(\"div_corr_norm\", \"correction.div_corr * normalization\") \\\n",
    "                                .Histo2D(model, \"k_th_x\", \"k_th_y\", \"div_corr_norm\");\n",
    "\n",
    "\n",
    "    # normalize histograms\n",
    "    for bi in binnings:\n",
    "        bh_t_before[bi].Scale(1., \"width\");\n",
    "        bh_t_after_no_corr[bi].Scale(1., \"width\");\n",
    "        bh_t_after[bi].Scale(1., \"width\");\n",
    "        \n",
    "    hlist.append(h_timestamp_dgn) #0\n",
    "    hlist.append(h_y_L_1_F_vs_x_L_1_F_al_nosel) #1\n",
    "    hlist.append(h_y_L_2_N_vs_x_L_2_N_al_nosel)\n",
    "    hlist.append(h_y_L_2_F_vs_x_L_2_F_al_nosel)\n",
    "    hlist.append(h_y_R_1_F_vs_x_R_1_F_al_nosel)\n",
    "    hlist.append(h_y_R_2_N_vs_x_R_2_N_al_nosel)\n",
    "    hlist.append(h_y_R_2_F_vs_x_R_2_F_al_nosel) #6\n",
    "    hlist.append(h_timestamp_sel) #7\n",
    "    hlist.append(h_y_L_1_F_vs_x_L_1_F_noal_sel) #8\n",
    "    hlist.append(h_y_L_2_N_vs_x_L_2_N_noal_sel)\n",
    "    hlist.append(h_y_L_2_F_vs_x_L_2_F_noal_sel)\n",
    "    hlist.append(h_y_R_1_F_vs_x_R_1_F_noal_sel)\n",
    "    hlist.append(h_y_R_2_N_vs_x_R_2_N_noal_sel)\n",
    "    hlist.append(h_y_R_2_F_vs_x_R_2_F_noal_sel) #13\n",
    "    hlist.append(h_y_L_1_F_vs_x_L_1_F_al_sel) #14\n",
    "    hlist.append(h_y_L_2_N_vs_x_L_2_N_al_sel)\n",
    "    hlist.append(h_y_L_2_F_vs_x_L_2_F_al_sel)\n",
    "    hlist.append(h_y_R_1_F_vs_x_R_1_F_al_sel)\n",
    "    hlist.append(h_y_R_2_N_vs_x_R_2_N_al_sel)\n",
    "    hlist.append(h_y_R_2_F_vs_x_R_2_F_al_sel) #19\n",
    "    hlist.append(th_x_diffLR) #20\n",
    "    hlist.append(th_y_diffLR)\n",
    "    hlist.append(th_x_diffLF) #22\n",
    "    hlist.append(th_x_diffRF)\n",
    "    hlist.append(h_th_x_diffLR_vs_th_x) #24\n",
    "    hlist.append(h_th_y_diffLR_vs_th_y)\n",
    "    hlist.append(h_th_x_diffLR_vs_vtx_x)\n",
    "    hlist.append(h_th_y_L_vs_th_x_L) #27\n",
    "    hlist.append(h_th_y_R_vs_th_x_R)\n",
    "    hlist.append(h_th_y_vs_th_x)\n",
    "    hlist.append(h_th_y_L_vs_th_y_R) #30\n",
    "    hlist.append(h_th_x) #31\n",
    "    hlist.append(h_th_y)\n",
    "    hlist.append(h_th_y_flipped) #33\n",
    "    hlist.append(h_th_x_L) #34\n",
    "    hlist.append(h_th_x_R)\n",
    "    hlist.append(h_th_y_L) #36\n",
    "    hlist.append(h_th_y_R)\n",
    "    hlist.append(h_th_y_L_F) #38\n",
    "    hlist.append(h_th_y_L_N)\n",
    "    hlist.append(h_th_y_R_N)\n",
    "    hlist.append(h_th_y_R_F) #41\n",
    "    hlist.append(h_vtx_x) #42\n",
    "    hlist.append(h_vtx_x_L)\n",
    "    hlist.append(h_vtx_x_R) #44\n",
    "    hlist.append(h_vtx_y) #45\n",
    "    hlist.append(h_vtx_y_L)\n",
    "    hlist.append(h_vtx_y_R)\n",
    "    hlist.append(h_vtx_x_L_vs_vtx_x_R) #48\n",
    "    hlist.append(h_vtx_y_L_vs_vtx_y_R)\n",
    "    hlist.append(h_vtx_x_L_vs_th_x_L) #50\n",
    "    hlist.append(h_vtx_x_R_vs_th_x_R)\n",
    "    hlist.append(h_vtx_y_L_vs_th_y_L)\n",
    "    hlist.append(h_vtx_y_R_vs_th_y_R) #53\n",
    "    hlist.append(h_vtx_x_diffLR) #54\n",
    "    hlist.append(h_vtx_y_diffLR) \n",
    "    hlist.append(h_vtx_x_diffLR_vs_th_x) #56\n",
    "    hlist.append(h_vtx_y_diffLR_vs_th_y)\n",
    "    hlist.append(h_vtx_x_diffLR_vs_vtx_x_R) #58\n",
    "    hlist.append(h_vtx_y_diffLR_vs_vtx_y_R)\n",
    "\n",
    "    for bi in binnings:\n",
    "        hlist.append(bh_t_Nev_before[bi]) # 60, 62, 64\n",
    "        hlist.append(bh_t_before[bi]) #61, 63, 65\n",
    "    hlist.append(h_th_y_vs_th_x_before) # 60 + len(binnings) * 2 (should be 66 here)\n",
    "    for bi in binnings:\n",
    "        hlist.append(bh_t_Nev_after_no_corr[bi]) # 61 + len(binnings) * 2 (should be 67 here)\n",
    "        hlist.append(bh_t_after_no_corr[bi])\n",
    "        hlist.append(bh_t_after[bi])\n",
    "    hlist.append(h_th_y_vs_th_x_after) # 60 + len(binnings)*5 + 1\n",
    "    hlist.append(h_th_vs_phi_after) \n",
    "    hlist.append(bh_t_normalized_ob_1_30_02)\n",
    "    hlist.append(h_th_y_vs_th_x_normalized)    \n",
    "\n",
    "    return hlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our code is basically following the same structure as the original version with two main functions: `distill` and `distributions`. Unlike the reference code, the creation of an intermediate ROOT file with a reduced dataset is no longer needed since running in parallel will shorten the execution time.\n",
    "\n",
    "In order to go distributed, we need to encapsulate our analysis in a function that receives a `RDataFrame`. Thus, we define the `fillHist` function, whose body corresponds to the `RDataFrame` analysis we already saw in the local execution example. Note how the function returns the list of histogram produced during the analysis.\n",
    "\n",
    "As more than one histogram is produced during `distributions`, we need to append all of them to a list so they can be reachable at the reduction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 40\n",
    "\n",
    "def fillHist(rdf):\n",
    "    distilled = distill(rdf)\n",
    "    return distributions(distilled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current implementation, a second function is needed to merge partial results - in this case, a **list** of partially filled histograms coming from the distributed parallelization. This requirement will be removed for the programming model to be simpler, and the implementation will automatically take care of the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeHist(l1, l2):\n",
    "    [first.Add(second) for (first, second) in zip(l1, l2)]\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell launches the distributed computation with Spark, where a `DistTree` is constructed from three parameters:\n",
    "- List of files of our dataset\n",
    "- Tree name\n",
    "- Desired number of partitions of our dataset (will potentially determine the amount of parallelism we will achieve)\n",
    "\n",
    "Once the `DistTree` is created, we invoke the `ProcessAndMerge` method on it to trigger the distributed processing. The two arguments of this call are the two functions defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original analysis skips some input files, hence we use input_files*.txt files\n",
    "# to specify the subset of processed input files. \n",
    "source_file       = \"input_files_{}.txt\".format(selected_ds)\n",
    "\n",
    "# Name of the ROOT Tree\n",
    "input_ntuple_name = \"TotemNtuple\"\n",
    "\n",
    "# Remote path in EOS\n",
    "prefix            = \"root://eostotem//eos/totem/data/cmstotem/2015/90m/Totem/Ntuple/version2/{}/\".format(DS[selected_ds])\n",
    "\n",
    "# Construct list of prefix + input files\n",
    "input_files       = [prefix + line.rstrip('\\r\\n') for line in open(source_file)]\n",
    "\n",
    "dTree = DistTree(input_files,\n",
    "                 treename = \"TotemNtuple\",\n",
    "                 npartitions = 4)\n",
    "\n",
    "histos = dTree.ProcessAndMerge(fillHist, mergeHist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if the everything run successfully, the analysis will be completed. \n",
    "\n",
    "It is worthwhile to mention that from this cell on, every execution will be local again. This is interesting since now we are able to select which parts of our analysis will run in local or distributed on the same notebook, thus reducing execution times while keeping an interactive session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results\n",
    "\n",
    "\n",
    "As a result of `ProccessAndMerge` a list of already merged histograms is stored in `histos`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%jsroot on\n",
    "c = ROOT.TCanvas();\n",
    "histos[75].Draw(\"colz\")\n",
    "ROOT.gPad.SetLogy(); # Set Y-axis as logarithmic scale\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results\n",
    "\n",
    "Finally, we can save the histograms on a ROOT file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histos[21].Scale(1., \"width\");\n",
    "histos[20].Scale(1., \"width\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save histograms\n",
    "c = ROOT.TCanvas();\n",
    "outF = ROOT.TFile.Open(\"distributions_\" + selected_diagonal + \".root\", \"recreate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = outF.mkdir(\"metadata\");\n",
    "c = ROOT.TCanvas(\"rate cmp\");\n",
    "histos[0].Draw();\n",
    "#h_timestamp_B0.SetLineColor(4);\n",
    "#h_timestamp_B0.Draw(\"sames\");\n",
    "histos[7].SetLineColor(2);\n",
    "histos[7].Draw(\"sames\");\n",
    "c.Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Line 1700\n",
    "hitDistDir = outF.mkdir(\"hit distributions\");\n",
    "ROOT.gDirectory = hitDistDir.mkdir(\"vertical, aligned, before selection\");\n",
    "histos[3].Write();\n",
    "histos[2].Write();\n",
    "histos[1].Write();\n",
    "histos[4].Write();\n",
    "histos[5].Write();\n",
    "histos[6].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = hitDistDir.mkdir(\"vertical, not aligned, after selection\");\n",
    "histos[10].Write();\n",
    "histos[9].Write();\n",
    "histos[8].Write();\n",
    "histos[11].Write();\n",
    "histos[12].Write();\n",
    "histos[13].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = hitDistDir.mkdir(\"vertical, aligned, after selection\");\n",
    "histos[14].Write();\n",
    "histos[15].Write();\n",
    "histos[16].Write();\n",
    "histos[17].Write();\n",
    "histos[18].Write();\n",
    "histos[19].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = outF.mkdir(\"selected - hits\");\n",
    "ROOT.gDirectory = outF.mkdir(\"selected - angles\");\n",
    "\n",
    "histos[20].Sumw2(); histos[20].Write()\n",
    "histos[21].Sumw2(); histos[21].Write()\n",
    "\n",
    "histos[22].Sumw2(); histos[22].Write()\n",
    "histos[23].Sumw2(); histos[23].Write()\n",
    "\n",
    "histos[24].Write();\n",
    "histos[25].Write();\n",
    "histos[26].Write();\n",
    "\n",
    "histos[27].Write();\n",
    "histos[28].Write();\n",
    "histos[29].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas();\n",
    "c.SetLogz(1);\n",
    "c.ToggleEventStatus();\n",
    "c.SetCrosshair(1);\n",
    "histos[30].Draw(\"colz\");\n",
    "#g_th_y_L_vs_th_y_R.Draw(\"p\");\n",
    "c.Write(\"canvas_th_y_L_vs_th_y_R\");\n",
    "\n",
    "histos[31].SetLineColor(1); histos[31].Write();\n",
    "histos[32].SetLineColor(1); histos[32].Write();\n",
    "histos[33].SetLineColor(1); histos[33].Write();\n",
    "\n",
    "histos[34].SetLineColor(2); histos[34].Write();\n",
    "histos[35].SetLineColor(4); histos[35].Write();\n",
    "\n",
    "histos[36].SetLineColor(2); histos[36].Write();\n",
    "histos[37].SetLineColor(4); histos[37].Write();\n",
    "\n",
    "histos[38].SetLineColor(2); histos[38].Write();\n",
    "histos[39].SetLineColor(6); histos[39].Write();\n",
    "histos[40].SetLineColor(4); histos[40].Write();\n",
    "histos[41].SetLineColor(7); histos[41].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = outF.mkdir(\"selected - vertex\");\n",
    "histos[42].SetLineColor(1); histos[42].Write();\n",
    "histos[43].SetLineColor(2); histos[43].Write();\n",
    "histos[44].SetLineColor(4); histos[44].Write();\n",
    "\n",
    "histos[45].SetLineColor(1); histos[45].Write();\n",
    "histos[46].SetLineColor(2); histos[46].Write();\n",
    "histos[47].SetLineColor(4); histos[47].Write();\n",
    "\n",
    "histos[48].Write();\n",
    "histos[49].Write();\n",
    "\n",
    "histos[50].Write();\n",
    "histos[51].Write();\n",
    "histos[52].Write();\n",
    "histos[53].Write();\n",
    "\n",
    "histos[54].Sumw2(); histos[54].Write();\n",
    "histos[55].Sumw2(); histos[55].Write();\n",
    "\n",
    "histos[56].Write();\n",
    "histos[57].Write();\n",
    "\n",
    "histos[58].Write();\n",
    "histos[59].Write();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accDir = outF.mkdir(\"acceptance correction\");\n",
    "i = 0\n",
    "for bi in binnings:\n",
    "    ROOT.gDirectory = accDir.mkdir(bi);\n",
    "    #bh_t_Nev_before[bi].Sumw2();        bh_t_Nev_before[bi].Write();\n",
    "    histos[60+i*2].Sumw2();        histos[60+i*2].Write();\n",
    "    #bh_t_Nev_after_no_corr[bi].Sumw2(); bh_t_Nev_after_no_corr[bi].Write();\n",
    "    histos[61+len(binnings)*2 + i*3].Sumw2(); histos[61+len(binnings)*2 + i*3].Write();\n",
    "    #bh_t_before[bi].Sumw2();            bh_t_before[bi].Write();\n",
    "    histos[61+i*2].Sumw2();            histos[61+i*2].Write();\n",
    "    #bh_t_after_no_corr[bi].Sumw2();     bh_t_after_no_corr[bi].Write();\n",
    "    histos[62+len(binnings)*2 + i*3].Sumw2();     histos[62+len(binnings)*2 + i*3].Write();\n",
    "    #bh_t_after[bi].Sumw2();             bh_t_after[bi].Write();\n",
    "    histos[63+len(binnings)*2 + i*3].Sumw2();             histos[63+len(binnings)*2 + i*3].Write();\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROOT.gDirectory = accDir;\n",
    "\n",
    "histos[60 + len(binnings)*2].Sumw2();     histos[60 + len(binnings)*2].Write();\n",
    "histos[60 + len(binnings)*5 + 1].Sumw2(); histos[60 + len(binnings)*5 + 1].Write();\n",
    "histos[60 + len(binnings)*5 + 2].Sumw2(); histos[60 + len(binnings)*5 + 2].Write();\n",
    "\n",
    "normDir = outF.mkdir(\"normalization\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.executor.extraLibraryPath",
     "value": "{LD_LIBRARY_PATH}"
    },
    {
     "name": "spark.yarn.dist.files",
     "value": "./DistROOT.py,./common_definitions.h,./common_algorithms.h,./parameters.h,./parameters_global.h,./common.h,./common_cuts.h,./common_parameters.h,./binning/generators.root"
    },
    {
     "name": "spark.executor.instances",
     "value": "4"
    },
    {
     "name": "spark.driver.memory",
     "value": "4g"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
